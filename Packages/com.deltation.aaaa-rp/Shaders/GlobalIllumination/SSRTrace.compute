#pragma kernel CS

#include "Packages/com.deltation.aaaa-rp/ShaderLibrary/Depth.hlsl"
#include "Packages/com.deltation.aaaa-rp/ShaderLibrary/CameraDepth.hlsl"
#include "Packages/com.deltation.aaaa-rp/ShaderLibrary/CameraHZB.hlsl"
#include "Packages/com.deltation.aaaa-rp/ShaderLibrary/GBuffer.hlsl"
#include "Packages/com.deltation.aaaa-rp/Runtime/Passes/GlobalIllumination/SSR/SSRComputeShaders.cs.hlsl"

#define THREAD_GROUP_SIZE TRACE_THREAD_GROUP_SIZE

float4x4 _SSR_ViewProjMatrix;
float4x4 _SSR_InvViewProjMatrix;
float4   _SSR_CameraPosition;
float4   _SSR_ScreenSize;

RWTexture2D<float4> _Result;

float2 PixelUVToNDC(const float2 pixelUV)
{
    float2 result = pixelUV;
    result = result * 2 - 1;
    #ifdef UNITY_UV_STARTS_AT_TOP
    result.y *= -1;
    #endif
    return result;
}

float2 NDCToPixelUV(const float2 ndc)
{
    float2 result = ndc;
    #ifdef UNITY_UV_STARTS_AT_TOP
    result.y *= -1;
    #endif
    result = result * 0.5 + 0.5;
    return result;
}

#define CELL_STEP_OFFSET 0.05

void StepThroughCell(inout float3 raySampleSS, const float3 reflectionSS, const int mipLevel)
{
    // Size of current mip 
    int2 mipSize = _CameraHZBMipRects[mipLevel].zw;

    // UV converted to index in the mip
    float2 mipCellIndex = raySampleSS.xy * float2(mipSize);

    //
    // Find the cell boundary UV based on the direction of the ray
    // Take floor() or ceil() depending on the sign of RayDir.xy
    //
    float2 boundaryUV;
    boundaryUV.x = reflectionSS.x > 0 ? ceil(mipCellIndex.x) / float(mipSize.x) : floor(mipCellIndex.x) / float(mipSize.x);
    boundaryUV.y = reflectionSS.y > 0 ? ceil(mipCellIndex.y) / float(mipSize.y) : floor(mipCellIndex.y) / float(mipSize.y);

    //
    // We can now represent the cell boundary as being formed by the intersection of 
    // two lines which can be represented by 
    //
    // x = BoundaryUV.x
    // y = BoundaryUV.y
    //
    // Intersect the parametric equation of the Ray with each of these lines
    //
    float2 t;
    t.x = (boundaryUV.x - raySampleSS.x) / reflectionSS.x;
    t.y = (boundaryUV.y - raySampleSS.y) / reflectionSS.y;

    // Pick the cell intersection that is closer, and march to that cell
    if (abs(t.x) < abs(t.y))
    {
        raySampleSS += (t.x + CELL_STEP_OFFSET / mipSize.x) * reflectionSS;
    }
    else
    {
        raySampleSS += (t.y + CELL_STEP_OFFSET / mipSize.y) * reflectionSS;
    }
}

#define NUM_BINARY_SEARCH_SAMPLES 16

float3 BinarySearchHZB(const float3 raySampleSS, const float3 reflectionSS, const float startZBufferValue, const float mipLevel)
{
    float  t = abs(raySampleSS.z - startZBufferValue) / reflectionSS.z;
    float3 minRaySampleSS = raySampleSS - reflectionSS * t;
    float3 maxRaySampleSS = raySampleSS;
    float3 midRaySampleSS;

    for (int i = 0; i < NUM_BINARY_SEARCH_SAMPLES; i++)
    {
        midRaySampleSS = lerp(minRaySampleSS, maxRaySampleSS, 0.5);
        const float2 pixelCoord = float2(raySampleSS.xy * _SSR_ScreenSize.xy);
        const float  zBufferValue = CameraHZB::LoadClamp(pixelCoord, mipLevel);

        if (GREATER_DEPTH(midRaySampleSS.z, zBufferValue))
        {
            maxRaySampleSS = midRaySampleSS;
        }
        else
        {
            minRaySampleSS = midRaySampleSS;
        }
    }

    return midRaySampleSS;
}


bool GetReflection(
    const float3 reflectionSS,
    const float3 positionSS,
    out float3   result)
{
    result = 0;

    float2 uvSamplingAttenuation = 0;
    float3 raySampleSS = positionSS + reflectionSS * 0.001f;
    int    mipLevel = 0;

    int loop = 0;

    UNITY_LOOP
    while (mipLevel < _CameraHZBLevelCount && loop < 1024)
    {
        ++loop;
        StepThroughCell(raySampleSS, reflectionSS, mipLevel);

        // Attenuate samples near screen edges.
        const float uvEdge = 0.025f;
        uvSamplingAttenuation = smoothstep(0.0, uvEdge, raySampleSS.xy) * (1 - smoothstep(1 - uvEdge, 1, raySampleSS.xy));

        UNITY_BRANCH
        if (any(uvSamplingAttenuation))
        {
            const float2 pixelCoord = float2(raySampleSS.xy * _SSR_ScreenSize.xy);
            const float  zBufferValue = CameraHZB::LoadClamp(pixelCoord, mipLevel);

            UNITY_BRANCH
            if (LESS_DEPTH(raySampleSS.z, zBufferValue))
            {
                ++mipLevel;
                mipLevel = min(mipLevel, _CameraHZBLevelCount - 1);
            }
            else
            {
                if (mipLevel == 0)
                {
                    // if (zBufferValue == UNITY_RAW_FAR_CLIP_VALUE)
                    // {
                    //     return false;
                    // }

                    // raySampleSS = BinarySearchHZB(raySampleSS, reflectionSS, zBufferValue, mipLevel);
                    result = float3(raySampleSS.xy, uvSamplingAttenuation.x * uvSamplingAttenuation.y);
                    return true;
                }

                const float t = (raySampleSS.z - zBufferValue) / reflectionSS.z;
                raySampleSS -= reflectionSS * t;

                --mipLevel;
            }
        }
        else
        {
            // Went past screen bounds.
            return false;
        }
    }

    result = float3(raySampleSS.xy, uvSamplingAttenuation.x * uvSamplingAttenuation.y);
    return true;
}

float2 Cell(float2 ray, float2 cellCount)
{
    return floor(ray.xy * cellCount);
}

float2 CellCount(float level)
{
    return _ScreenSize.xy / (level == 0.0 ? 1.0 : exp2(level));
    // return (uint)_ScreenSize.xy >> (uint)level;
}

float3 IntersectCellBoundary(float3 pos, float3 dir, float2 cellID, float2 cellCount, float2 crossStep, float2 crossOffset)
{
    float2 cellSize = 1.0 / cellCount;
    float2 planes = cellID / cellCount + cellSize * crossStep;

    float2 solutions = (planes - pos.xy) / dir.xy;
    float3 intersectionPos = pos + dir * min(solutions.x, solutions.y);

    intersectionPos.xy += (solutions.x < solutions.y) ? float2(crossOffset.x, 0.0) : float2(0.0, crossOffset.y);

    return intersectionPos;
}

bool CrossedCellBoundary(float2 cellID0, float2 cellID1)
{
    return (int)cellID0.x != (int)cellID1.x || (int)cellID0.y != (int)cellID1.y;
}

float MinimumDepthPlane(float2 ray, float level, float2 cell_count)
{
    return CameraHZB::LoadClamp(ray * cell_count, level);
}

#define HIZ_START_LEVEL 0
#define HIZ_MAX_LEVEL (_CameraHZBLevelCount - 1)
#define HIZ_STOP_LEVEL 0
#define MAX_ITERATIONS 512

float3 hi_z_trace(const float3 p, const float3 v, out uint iterations)
{
    float  level = HIZ_START_LEVEL;
    float3 v_z = v / v.z;
    float2 hi_z_size = CellCount(level);
    float3 ray = p;

    float2 cross_step = float2(v.x >= 0.0 ? 1.0 : -1.0, v.y >= 0.0 ? 1.0 : -1.0);
    float2 cross_offset = cross_step * 0.00001;
    cross_step = saturate(cross_step);

    float2 ray_cell = Cell(ray.xy, hi_z_size.xy);
    ray = IntersectCellBoundary(ray, v, ray_cell, hi_z_size, cross_step, cross_offset);

    iterations = 0;
    while (level >= HIZ_STOP_LEVEL && iterations < MAX_ITERATIONS)
    {
        // get the cell number of the current ray
        float2 current_cell_count = CellCount(level);
        float2 old_cell_id = Cell(ray.xy, current_cell_count);

        // get the minimum depth plane in which the current ray resides
        float min_z = MinimumDepthPlane(ray.xy, level, current_cell_count);

        // intersect only if ray depth is below the minimum depth plane
        float3 tmp_ray = ray;
        if (GREATER_DEPTH(0, v.z))
        {
            float min_minus_ray = min_z - ray.z;
            tmp_ray = GREATER_DEPTH(min_minus_ray, 0) ? ray + v_z * min_minus_ray : tmp_ray;
            float2 new_cell_id = Cell(tmp_ray.xy, current_cell_count);
            if (CrossedCellBoundary(old_cell_id, new_cell_id))
            {
                tmp_ray = IntersectCellBoundary(ray, v, old_cell_id, current_cell_count, cross_step, cross_offset);
                level = min(HIZ_MAX_LEVEL, level + 2.0f);
            }
            else
            {
                if (level == 1 && abs(min_minus_ray) > 0.0001)
                {
                    tmp_ray = IntersectCellBoundary(ray, v, old_cell_id, current_cell_count, cross_step, cross_offset);
                    level = 2;
                }
            }
        }
        else if (LESS_DEPTH(ray.z, min_z))
        {
            tmp_ray = IntersectCellBoundary(ray, v, old_cell_id, current_cell_count, cross_step, cross_offset);
            level = min(HIZ_MAX_LEVEL, level + 2.0f);
        }

        ray.xyz = tmp_ray.xyz;
        --level;

        ++iterations;
    }
    return ray;
}

[numthreads(THREAD_GROUP_SIZE, THREAD_GROUP_SIZE, 1)]
void CS(const uint3 dispatchThreadID : SV_DispatchThreadID)
{
    const uint2 pixelCoords = dispatchThreadID.xy;

    UNITY_BRANCH
    if (all((float2)pixelCoords < _SSR_ScreenSize.xy))
    {
        const float2 pixelUV = (pixelCoords.xy) * _SSR_ScreenSize.zw;
        const float2 positionNDC = PixelUVToNDC(pixelUV);
        const float  deviceZ = LoadDeviceDepth(pixelCoords);

        float4 positionWS = mul(_SSR_InvViewProjMatrix, float4(positionNDC, deviceZ, 1));
        positionWS /= positionWS.w;

        const float3 eyeWS = normalize(positionWS.xyz - _SSR_CameraPosition.xyz);
        const float3 normalWS = UnpackGBufferNormal(LOAD_TEXTURE2D(_GBuffer_Normals, pixelCoords).xy);

        const float4 positionSS = float4(pixelUV, deviceZ, 1.0f);
        const float3 reflectionWS = reflect(eyeWS, normalWS);

        // This will check the direction of the reflection vector with the view direction,
        // and if they are pointing in the same direction, it will drown out those reflections 
        // since we are limited to pixels visible on screen. Attenuate reflections for angles between 
        // 60 degrees and 75 degrees, and drop all contribution beyond the (-60,60)  degree range
        float cameraFacingReflectionAttenuation = 1 - smoothstep(0.25, 0.5, dot(-eyeWS, reflectionWS));

        // Reject if the reflection vector is pointing back at the viewer.
        UNITY_BRANCH
        if (cameraFacingReflectionAttenuation <= 0)
        {
            _Result[pixelCoords] = 0;
        }
        else
        {
            const float3 reflectionPointWS = 10.0f * reflectionWS + positionWS.xyz;

            float4 reflectionPointSS = mul(_SSR_ViewProjMatrix, float4(reflectionPointWS, 1));
            reflectionPointSS /= reflectionPointSS.w;
            reflectionPointSS.xy = NDCToPixelUV(reflectionPointSS.xy);

            const float3 reflectionSS = normalize(reflectionPointSS.xyz - positionSS.xyz);

            // const bool hit = GetReflection(reflectionSS, positionSS.xyz, reflectionUVAttenuation);
            float  attenuation = cameraFacingReflectionAttenuation;
            uint   iterations;
            float3 hit = hi_z_trace(positionSS.xyz + reflectionSS * 0.01f, reflectionSS, iterations);

            UNITY_BRANCH
            if (SampleDeviceDepth(hit.xy) == UNITY_RAW_FAR_CLIP_VALUE)
            {
                attenuation = 0;
            }
            else
            {
                // This will check the direction of the normal of the reflection sample with the
                // direction of the reflection vector, and if they are pointing in the same direction,
                // it will drown out those reflections since backward facing pixels are not available 
                // for screen space reflection. Attenuate reflections for angles between 90 degrees 
                // and 100 degrees, and drop all contribution beyond the (-100,100)  degree range
                const float2 packedGBufferNormals = SAMPLE_TEXTURE2D_LOD(_GBuffer_Normals, sampler_LinearClamp, hit.xy, 0).xy;
                const float3 reflectionNormalWS = UnpackGBufferNormal(packedGBufferNormals);
                const float  directionBasedAttenuation = smoothstep(-0.17, 0.0, dot(reflectionNormalWS.xyz, -reflectionWS));
                attenuation *= directionBasedAttenuation;

                // Attenuate samples near screen edges.
                const float  uvEdge = 0.025f;
                const float2 uvSamplingAttenuation = smoothstep(0.0, uvEdge, hit.xy) * (1 - smoothstep(1 - uvEdge, 1, hit.xy));
                attenuation *= uvSamplingAttenuation.x * uvSamplingAttenuation.y;
            }

            _Result[pixelCoords] = float4(hit.xy, 0, attenuation);

            // if (hit)
            // {
            //     attenuation *= reflectionUVAttenuation.z;
            //
            //     // This will check the direction of the normal of the reflection sample with the
            //     // direction of the reflection vector, and if they are pointing in the same direction,
            //     // it will drown out those reflections since backward facing pixels are not available 
            //     // for screen space reflection. Attenuate reflections for angles between 90 degrees 
            //     // and 100 degrees, and drop all contribution beyond the (-100,100)  degree range
            //     const float2 packedGBufferNormals = SAMPLE_TEXTURE2D_LOD(_GBuffer_Normals, sampler_LinearClamp, reflectionUVAttenuation.xy, 0).xy;
            //     const float3 reflectionNormalWS = UnpackGBufferNormal(packedGBufferNormals);
            //     const float  directionBasedAttenuation = smoothstep(-0.17, 0.0, dot(reflectionNormalWS.xyz, -reflectionWS));
            //     attenuation *= directionBasedAttenuation;
            // }
            //
            // _Result[pixelCoords] = hit ? float4(reflectionUVAttenuation.xy, 1, attenuation) : float4(0, 0, 0, 0);
        }
    }
}