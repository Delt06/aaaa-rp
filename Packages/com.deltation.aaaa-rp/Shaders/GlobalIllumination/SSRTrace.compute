#pragma kernel CS

#include "Packages/com.deltation.aaaa-rp/ShaderLibrary/Depth.hlsl"
#include "Packages/com.deltation.aaaa-rp/ShaderLibrary/CameraDepth.hlsl"
#include "Packages/com.deltation.aaaa-rp/ShaderLibrary/CameraHZB.hlsl"
#include "Packages/com.deltation.aaaa-rp/ShaderLibrary/GBuffer.hlsl"
#include "Packages/com.deltation.aaaa-rp/Runtime/Passes/GlobalIllumination/SSR/SSRComputeShaders.cs.hlsl"

#define THREAD_GROUP_SIZE TRACE_THREAD_GROUP_SIZE

float4x4 _SSR_ViewProjMatrix;
float4x4 _SSR_InvViewProjMatrix;
float4   _SSR_CameraPosition;
float4   _SSR_ScreenSize;

RWTexture2D<float4> _Result;

float2 PixelUVToNDC(const float2 pixelUV)
{
    float2 result = pixelUV;
    result = result * 2 - 1;
    #ifdef UNITY_UV_STARTS_AT_TOP
    result.y *= -1;
    #endif
    return result;
}

float2 NDCToPixelUV(const float2 ndc)
{
    float2 result = ndc;
    #ifdef UNITY_UV_STARTS_AT_TOP
    result.y *= -1;
    #endif
    result = result * 0.5 + 0.5;
    return result;
}

#define CELL_STEP_OFFSET 0.05

void StepThroughCell(inout float3 raySampleSS, const float3 reflectionSS, const int mipLevel)
{
    // Size of current mip 
    int2 mipSize = _CameraHZBMipRects[mipLevel].zw;

    // UV converted to index in the mip
    float2 mipCellIndex = raySampleSS.xy * float2(mipSize) ;

    //
    // Find the cell boundary UV based on the direction of the ray
    // Take floor() or ceil() depending on the sign of RayDir.xy
    //
    float2 boundaryUV;
    boundaryUV.x = reflectionSS.x > 0 ? ceil(mipCellIndex.x) / float(mipSize.x) : floor(mipCellIndex.x) / float(mipSize.x);
    boundaryUV.y = reflectionSS.y > 0 ? ceil(mipCellIndex.y) / float(mipSize.y) : floor(mipCellIndex.y) / float(mipSize.y);

    //
    // We can now represent the cell boundary as being formed by the intersection of 
    // two lines which can be represented by 
    //
    // x = BoundaryUV.x
    // y = BoundaryUV.y
    //
    // Intersect the parametric equation of the Ray with each of these lines
    //
    float2 t;
    t.x = (boundaryUV.x - raySampleSS.x) / reflectionSS.x;
    t.y = (boundaryUV.y - raySampleSS.y) / reflectionSS.y;

    // Pick the cell intersection that is closer, and march to that cell
    if (abs(t.x) < abs(t.y))
    {
        raySampleSS += (t.x + CELL_STEP_OFFSET / mipSize.x) * reflectionSS;
    }
    else
    {
        raySampleSS += (t.y + CELL_STEP_OFFSET / mipSize.y) * reflectionSS;
    }
}

#define NUM_BINARY_SEARCH_SAMPLES 16

float3 BinarySearchHZB(const float3 raySampleSS, const float3 reflectionSS, const float startZBufferValue, const float mipLevel)
{
    float  t = abs(raySampleSS.z - startZBufferValue) / reflectionSS.z;
    float3 minRaySampleSS = raySampleSS - reflectionSS * t;
    float3 maxRaySampleSS = raySampleSS;
    float3 midRaySampleSS;

    for (int i = 0; i < NUM_BINARY_SEARCH_SAMPLES; i++)
    {
        midRaySampleSS = lerp(minRaySampleSS, maxRaySampleSS, 0.5);
        const float2 pixelCoord = float2(raySampleSS.xy * _SSR_ScreenSize.xy);
        const float  zBufferValue = CameraHZB::LoadClamp(pixelCoord, mipLevel);

        if (GREATER_DEPTH(midRaySampleSS.z, zBufferValue))
        {
            maxRaySampleSS = midRaySampleSS;
        }
        else
        {
            minRaySampleSS = midRaySampleSS;
        }
    }

    return midRaySampleSS;
}


bool GetReflection(
    const float3 reflectionSS,
    const float3 positionSS,
    out float3   result)
{
    result = 0;

    float2 uvSamplingAttenuation = 0;
    float3 raySampleSS = positionSS + reflectionSS * 0.001f;
    int    mipLevel = 0;

    int loop = 0;

    UNITY_LOOP
    while (mipLevel < _CameraHZBLevelCount && loop < 1024)
    {
        ++loop;
        StepThroughCell(raySampleSS, reflectionSS, mipLevel);

        // Attenuate samples near screen edges.
        const float uvEdge = 0.025f;
        uvSamplingAttenuation = smoothstep(0.0, uvEdge, raySampleSS.xy) * (1 - smoothstep(1 - uvEdge, 1, raySampleSS.xy));

        UNITY_BRANCH
        if (any(uvSamplingAttenuation))
        {
            const float2 pixelCoord = float2(raySampleSS.xy * _SSR_ScreenSize.xy);
            const float  zBufferValue = CameraHZB::LoadClamp(pixelCoord, mipLevel);

            UNITY_BRANCH
            if (LESS_DEPTH(raySampleSS.z, zBufferValue))
            {
                ++mipLevel;
                mipLevel = min(mipLevel, _CameraHZBLevelCount - 1);
            }
            else
            {
                if (mipLevel == 0)
                {
                    // if (zBufferValue == UNITY_RAW_FAR_CLIP_VALUE)
                    // {
                    //     return false;
                    // }

                    // raySampleSS = BinarySearchHZB(raySampleSS, reflectionSS, zBufferValue, mipLevel);
                    result = float3(raySampleSS.xy, uvSamplingAttenuation.x * uvSamplingAttenuation.y);
                    return true;
                }

                const float t = (raySampleSS.z - zBufferValue) / reflectionSS.z;
                raySampleSS -= reflectionSS * t;

                --mipLevel;
            }
        }
        else
        {
            // Went past screen bounds.
            return false;
        }
    }

    result = float3(raySampleSS.xy, uvSamplingAttenuation.x * uvSamplingAttenuation.y);
    return true;
}

[numthreads(THREAD_GROUP_SIZE, THREAD_GROUP_SIZE, 1)]
void CS(const uint3 dispatchThreadID : SV_DispatchThreadID)
{
    const uint2 pixelCoords = dispatchThreadID.xy;

    UNITY_BRANCH
    if (all((float2)pixelCoords < _SSR_ScreenSize.xy))
    {
        const float2 pixelUV = (pixelCoords.xy + 0.5) * _SSR_ScreenSize.zw;
        const float2 positionNDC = PixelUVToNDC(pixelUV);
        const float  deviceZ = LoadDeviceDepth(pixelCoords);

        float4 positionWS = mul(_SSR_InvViewProjMatrix, float4(positionNDC, deviceZ, 1));
        positionWS /= positionWS.w;

        const float3 eyeWS = normalize(positionWS.xyz - _SSR_CameraPosition.xyz);
        const float3 normalWS = UnpackGBufferNormal(LOAD_TEXTURE2D(_GBuffer_Normals, pixelCoords).xy);

        const float4 positionSS = float4(pixelUV, deviceZ, 1.0f);
        const float3 reflectionWS = reflect(eyeWS, normalWS);

        // This will check the direction of the reflection vector with the view direction,
        // and if they are pointing in the same direction, it will drown out those reflections 
        // since we are limited to pixels visible on screen. Attenuate reflections for angles between 
        // 60 degrees and 75 degrees, and drop all contribution beyond the (-60,60)  degree range
        float cameraFacingReflectionAttenuation = 1 - smoothstep(0.25, 0.5, dot(-eyeWS, reflectionWS));

        // Reject if the reflection vector is pointing back at the viewer.
        UNITY_BRANCH
        if (cameraFacingReflectionAttenuation <= 0)
        {
            _Result[pixelCoords] = 0;
        }
        else
        {
            const float3 reflectionPointWS = 10.0f * reflectionWS + positionWS.xyz;

            float4 reflectionPointSS = mul(_SSR_ViewProjMatrix, float4(reflectionPointWS, 1));
            reflectionPointSS /= reflectionPointSS.w;
            reflectionPointSS.xy = NDCToPixelUV(reflectionPointSS.xy);

            const float3 reflectionSS = normalize(reflectionPointSS.xyz - positionSS.xyz);

            float3     reflectionUVAttenuation;
            const bool hit = GetReflection(reflectionSS, positionSS.xyz, reflectionUVAttenuation);
            float      attenuation = cameraFacingReflectionAttenuation;

            if (hit)
            {
                attenuation *= reflectionUVAttenuation.z;

                // This will check the direction of the normal of the reflection sample with the
                // direction of the reflection vector, and if they are pointing in the same direction,
                // it will drown out those reflections since backward facing pixels are not available 
                // for screen space reflection. Attenuate reflections for angles between 90 degrees 
                // and 100 degrees, and drop all contribution beyond the (-100,100)  degree range
                const float2 packedGBufferNormals = SAMPLE_TEXTURE2D_LOD(_GBuffer_Normals, sampler_LinearClamp, reflectionUVAttenuation.xy, 0).xy;
                const float3 reflectionNormalWS = UnpackGBufferNormal(packedGBufferNormals);
                const float  directionBasedAttenuation = smoothstep(-0.17, 0.0, dot(reflectionNormalWS.xyz, -reflectionWS));
                attenuation *= directionBasedAttenuation;
            }

            _Result[pixelCoords] = hit ? float4(reflectionUVAttenuation.xy, 1, attenuation) : float4(0, 0, 0, 0);
        }
    }
}